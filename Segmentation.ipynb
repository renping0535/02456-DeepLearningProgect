{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!!!!!   Change the file path to your own   !!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping\n",
    "convert to gray image and check the corresponding class, then map them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "usage: check unique RGB combination and mask them with different class indices\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "anno = Image.open('/home/renping/02456 DeepLearning Project/train/cropped/anno/3_0_anno.png')\n",
    "anno_gray = Image.open('/home/renping/02456 DeepLearning Project/train/cropped/anno/3_0_anno.png').convert(\"L\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.suptitle(\"anno 3_0 RGB and gray\")\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(anno)\n",
    "# plt.title(\"3_0_anno_RGB\")\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(anno_gray)\n",
    "# plt.title(\"3_0_anno_gray\")\n",
    "# plt.show()\n",
    "\n",
    "print(anno_gray.mode)\n",
    "\n",
    "anno_arr = np.array(anno_gray)\n",
    "print(anno_arr)\n",
    "print(type(anno_arr))\n",
    "print(anno_arr.shape)\n",
    "print(np.unique(anno_arr))\n",
    "\n",
    "mapping = {\n",
    "    0: 0,\n",
    "    76: 0,\n",
    "    149: 1,\n",
    "    225: 2\n",
    "}\n",
    "# for k in mapping:\n",
    "#     print(k)\n",
    "for k in mapping:\n",
    "    anno_arr[anno_arr==k] = mapping[k]\n",
    "\n",
    "print(anno_arr)\n",
    "print(np.unique(anno_arr))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle(\"anno 3_0 RGB and gray\")\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(anno)\n",
    "plt.title(\"3_0_anno_RGB\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(anno_gray)\n",
    "plt.title(\"3_0_anno_gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(anno_arr)\n",
    "plt.title(\"3_0_mapping\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "usage: batch generator for train data\n",
    "input shape: (batch_size, num_channels, height, width)\n",
    "target shape: (batch_size, height, width)\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "class Custom_Data(data.Dataset):\n",
    "    def __init__(self, path_train, path_anno, transforms = None, normalize = None):\n",
    "        raw_img = os.listdir(path_train) # raw images\n",
    "        anno_img = os.listdir(path_anno) # anno images\n",
    "        self.raw_img = [os.path.join(path_train, img) for img in raw_img]\n",
    "        self.anno_img = [os.path.join(path_anno, img) for img in anno_img]\n",
    "        self.transform = transforms\n",
    "        # To Do: Data Augmentation #\n",
    "        self.mapping = {\n",
    "            0:0,\n",
    "            76:0,\n",
    "            149:1,\n",
    "            225:2,\n",
    "        }\n",
    "    def mask_to_class(self, mask): # \"mask\" is the anno image after converting to gray using \"L\" mode\n",
    "        for k in self.mapping:\n",
    "            mask[mask == k] = self.mapping[k]\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_img_path = self.raw_img[index]\n",
    "        raw_data = Image.open(raw_img_path)\n",
    "        raw_data = self.transform(raw_data)\n",
    "\n",
    "        anno_img_path = self.anno_img[index]\n",
    "        anno_data = Image.open(anno_img_path)\n",
    "\n",
    "        anno_data = self.transform(anno_data)\n",
    "        \n",
    "        anno_data = T.ToPILImage()(anno_data).convert(\"RGB\")\n",
    "        anno_data = anno_data.convert(\"L\")\n",
    "\n",
    "        anno_data = torch.from_numpy(np.array(anno_data)) # np.array -> torch tensor\n",
    "        anno_data = self.mask_to_class(anno_data)\n",
    "        \n",
    "        return raw_data, anno_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_img)\n",
    "\n",
    "# the path of your train data\n",
    "path_train = '/home/renping/02456 DeepLearning Project/train/cropped/raw'\n",
    "# the path of your corresponding annotated data\n",
    "path_anno = '/home/renping/02456 DeepLearning Project/train/cropped/anno'\n",
    "\n",
    "# normMean, normStd = Compute_Mean_Std(path_train)\n",
    "transform = T.Compose([\n",
    "    T.RandomCrop(256, padding=4),\n",
    "    T.RandomRotation(30),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "# norm = T.Normalize(normMean, normStd, inplace=True)\n",
    "\n",
    "train_data = Custom_Data(path_train, path_anno, transforms=transform, normalize=None)\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# train_iter = iter(train_loader)\n",
    "# raw_data, anno_data = train_iter.next()\n",
    "# print(\"raw dataset size = {}.\".format(raw_data.size()))\n",
    "# print(\"anno dataset size = {}.\".format(anno_data.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "traing and simple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from SegNet import SegNet\n",
    "from batch_generator import train_loader\n",
    "\n",
    "# initialize the writter\n",
    "log_dir=\"/home/renping/02456 DeepLearning Project/visualization\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# initialize the net, loss, optimizer\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"Running on GPU!\")\n",
    "    net = SegNet(input_channels=3, output_classes=3).cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    print(\"Running on CPU!\")\n",
    "    net = SegNet(input_channels=3, output_classes=3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epoch =20\n",
    "print(\"Start training!\")\n",
    "print(\"Total training epoch: {}.\".format(num_epoch))\n",
    "\n",
    "for epoch in range(0, num_epoch):\n",
    "    training_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        targets = targets.type(torch.int64)\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = training_loss+loss.item()\n",
    "        if (i%5 == 4): # check the loss for every 5 mini batch\n",
    "            print(\"epoch = {},mini batch = {} loss: {}\".format(epoch+1, i+1, loss.item()))\n",
    "    # check the loss for every epoch\n",
    "    print(\"epoch = {}, loss: {}\".format(epoch+1, training_loss))\n",
    "    writer.add_scalar(\"train/loss\", training_loss, epoch)\n",
    "writer.close()\n",
    "print(\"Finished Training\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"**********start predicting**********\")\n",
    "\n",
    "img = np.array(Image.open('/home/renping/02456 DeepLearning Project/test/cropped/raw/1_2_raw.png'))\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "img = torch.from_numpy(img).type(torch.FloatTensor).cuda()\n",
    "img = img.permute(2,0,1)\n",
    "print(img.shape)\n",
    "\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "pred = net(img)\n",
    "\n",
    "print(pred.shape)\n",
    "\n",
    "pred = pred.squeeze()\n",
    "\n",
    "test = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "plt.imshow(test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
